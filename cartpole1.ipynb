{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0eWwpRX7AA8S",
        "outputId": "c2e8e330-4503-4383-e12d-3b4bdaf4785c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-3227070836>:68: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  states = torch.FloatTensor(states)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 50, Total Reward: 31.0, Epsilon: 0.778\n",
            "Episode 100, Total Reward: 12.0, Epsilon: 0.606\n",
            "Episode 150, Total Reward: 147.0, Epsilon: 0.471\n",
            "Episode 200, Total Reward: 43.0, Epsilon: 0.367\n",
            "Episode 250, Total Reward: 246.0, Epsilon: 0.286\n",
            "Episode 300, Total Reward: 117.0, Epsilon: 0.222\n",
            "Episode 350, Total Reward: 107.0, Epsilon: 0.173\n",
            "Episode 400, Total Reward: 204.0, Epsilon: 0.135\n",
            "Episode 450, Total Reward: 17.0, Epsilon: 0.105\n",
            "Episode 500, Total Reward: 34.0, Epsilon: 0.082\n",
            "Training complete. Model saved to dqn_cartpole.pth.\n",
            "Episode 50, Total Reward: -200.0, Epsilon: 0.778\n",
            "Episode 100, Total Reward: -200.0, Epsilon: 0.606\n",
            "Episode 150, Total Reward: -200.0, Epsilon: 0.471\n",
            "Episode 200, Total Reward: -200.0, Epsilon: 0.367\n",
            "Episode 250, Total Reward: -200.0, Epsilon: 0.286\n",
            "Episode 300, Total Reward: -200.0, Epsilon: 0.222\n",
            "Episode 350, Total Reward: -200.0, Epsilon: 0.173\n",
            "Episode 400, Total Reward: -200.0, Epsilon: 0.135\n",
            "Episode 450, Total Reward: -200.0, Epsilon: 0.105\n",
            "Episode 500, Total Reward: -152.0, Epsilon: 0.082\n",
            "Episode 550, Total Reward: -130.0, Epsilon: 0.063\n",
            "Episode 600, Total Reward: -135.0, Epsilon: 0.049\n",
            "Episode 650, Total Reward: -115.0, Epsilon: 0.038\n",
            "Episode 700, Total Reward: -118.0, Epsilon: 0.030\n",
            "Episode 750, Total Reward: -200.0, Epsilon: 0.023\n",
            "Episode 800, Total Reward: -139.0, Epsilon: 0.018\n",
            "Episode 850, Total Reward: -178.0, Epsilon: 0.014\n",
            "Episode 900, Total Reward: -200.0, Epsilon: 0.011\n",
            "Episode 950, Total Reward: -114.0, Epsilon: 0.010\n",
            "Episode 1000, Total Reward: -111.0, Epsilon: 0.010\n",
            "Training complete. Model saved to dqn_mountaincar.pth.\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://5ad8adae66ae21f28e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://5ad8adae66ae21f28e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import imageio\n",
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "def train_dqn(env_name, model_path, state_size, action_size, episodes=500):\n",
        "    env = gym.make(env_name)\n",
        "    model = QNetwork(state_size, action_size)\n",
        "    target_model = QNetwork(state_size, action_size)\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "    buffer = deque(maxlen=10000)\n",
        "\n",
        "    gamma = 0.99\n",
        "    batch_size = 64\n",
        "    epsilon = 1.0\n",
        "    epsilon_min = 0.01\n",
        "    epsilon_decay = 0.995\n",
        "    update_target_every = 10\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "                    action = model(state_tensor).argmax().item()\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            buffer.append((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                batch = random.sample(buffer, batch_size)\n",
        "                states, actions, rewards_, next_states, dones = zip(*batch)\n",
        "                states = torch.FloatTensor(states)\n",
        "                actions = torch.LongTensor(actions).unsqueeze(1)\n",
        "                rewards_ = torch.FloatTensor(rewards_).unsqueeze(1)\n",
        "                next_states = torch.FloatTensor(next_states)\n",
        "                dones = torch.BoolTensor(dones).unsqueeze(1)\n",
        "\n",
        "                q_values = model(states).gather(1, actions)\n",
        "                max_next_q = target_model(next_states).max(1)[0].unsqueeze(1)\n",
        "                target = rewards_ + gamma * max_next_q * (~dones)\n",
        "\n",
        "                loss = criterion(q_values, target)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "        if ep % update_target_every == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "        if (ep + 1) % 50 == 0:\n",
        "            print(f\"Episode {ep + 1}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Training complete. Model saved to {model_path}.\")\n",
        "\n",
        "\n",
        "train_dqn(\"CartPole-v1\", \"dqn_cartpole.pth\", 4, 2, episodes=500)\n",
        "train_dqn(\"MountainCar-v0\", \"dqn_mountaincar.pth\", 2, 3, episodes=1000)\n",
        "\n",
        "\n",
        "\n",
        "def run_agent(env_name, model, episodes=1, agent_type=\"DQN\", video_path=\"agent_video.mp4\"):\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    frames = []\n",
        "    total_reward = 0\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            frames.append(env.render())\n",
        "\n",
        "            if agent_type == \"Random\":\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "                    action = model(state_tensor).argmax().item()\n",
        "\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "\n",
        "    env.close()\n",
        "    imageio.mimsave(video_path, frames, fps=30)\n",
        "    return video_path, total_reward\n",
        "\n",
        "\n",
        "cartpole_model = QNetwork(4, 2)\n",
        "mountaincar_model = QNetwork(2, 3)\n",
        "\n",
        "if os.path.exists(\"dqn_cartpole.pth\"):\n",
        "    cartpole_model.load_state_dict(torch.load(\"dqn_cartpole.pth\", map_location=\"cpu\"))\n",
        "    cartpole_model.eval()\n",
        "\n",
        "if os.path.exists(\"dqn_mountaincar.pth\"):\n",
        "    mountaincar_model.load_state_dict(torch.load(\"dqn_mountaincar.pth\", map_location=\"cpu\"))\n",
        "    mountaincar_model.eval()\n",
        "\n",
        "\n",
        "def gradio_interface(game, agent_type, episodes):\n",
        "    env_name = \"CartPole-v1\" if game == \"CartPole\" else \"MountainCar-v0\"\n",
        "    model = cartpole_model if game == \"CartPole\" else mountaincar_model\n",
        "    model.eval()\n",
        "    video_path = f\"{game}_{agent_type}_demo.mp4\"\n",
        "    video_path, score = run_agent(env_name, model, episodes, agent_type, video_path)\n",
        "    return video_path, score\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.Dropdown([\"CartPole\", \"MountainCar\"], label=\"Select Game\"),\n",
        "        gr.Dropdown([\"DQN\", \"Random\"], label=\"Select Agent\"),\n",
        "        gr.Slider(minimum=1, maximum=20, value=5, step=1, label=\"Episodes\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Video(label=\"Gameplay\"),\n",
        "        gr.Number(label=\"Total Reward\")\n",
        "    ],\n",
        "    title=\" RL Agent Playground\",\n",
        "    description=\"Select a game and agent type, run episodes, and watch the result!\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kg1BpoCNAEXe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}